{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First Regression with Pytorch - Annie 2/23.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba4e0yulEKMw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "SO53T6AvEaok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Load Boston dataset\n",
        "  # TASK: what are the X and y values in this dataset?\n",
        "        # The x value represents the factors causing housing price, and the y-value represents the housing price.\n",
        "  X, y = load_boston(return_X_y=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeDaiLscFGzx",
        "outputId": "1b0cb19f-dd70-433f-d527-10dbb6247dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK: What is a class is python? What's it used for? How does inheritance work in python?\n",
        "      # A class is a group of code that is a blueprint for creating an object. Each has certain features and methods that make it unique.\n",
        "# TASK: Find out what StandardScaler does. \n",
        "      # It's a method that takes a value, subtracts it from the mean, and divides by the standard deviation, also called scaling to unit variance.\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Boston dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]"
      ],
      "metadata": {
        "id": "rIarM2UWEbpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # TASK: how many parameters does this network have?\n",
        "          # (13*64) + (64*32) + (32*1) = 2912 parameters\n",
        "          # I think it's supposed to have around 500...\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.ReLU(),\n",
        "      #nn.Linear(13, 64),\n",
        "      #nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "UtSRIGMOE2s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Boston dataset\n",
        "dataset = BostonDataset(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "45JDMNE0FAH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "# TASK: what is L1 loss? what other loss could we use?\n",
        "      # L1 loss is a function that determines the mean absolute error between the value and the target. \n",
        "      # You could also use MSELoss to find the mean squared difference between the value and the target.\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "_KcWPhtXFptR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.float()\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 10 == 0:\n",
        "        print('Loss after mini-batch %5d: %.3f' %\n",
        "              (i + 1, current_loss / 500))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WwWrz3NGGN-",
        "outputId": "e770096b-73b1-48ab-ac22-e1ca71dd1e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch     1: 0.053\n",
            "Loss after mini-batch    11: 0.478\n",
            "Loss after mini-batch    21: 0.407\n",
            "Loss after mini-batch    31: 0.432\n",
            "Loss after mini-batch    41: 0.465\n",
            "Loss after mini-batch    51: 0.478\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.033\n",
            "Loss after mini-batch    11: 0.442\n",
            "Loss after mini-batch    21: 0.443\n",
            "Loss after mini-batch    31: 0.461\n",
            "Loss after mini-batch    41: 0.448\n",
            "Loss after mini-batch    51: 0.473\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.041\n",
            "Loss after mini-batch    11: 0.451\n",
            "Loss after mini-batch    21: 0.446\n",
            "Loss after mini-batch    31: 0.430\n",
            "Loss after mini-batch    41: 0.460\n",
            "Loss after mini-batch    51: 0.451\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.037\n",
            "Loss after mini-batch    11: 0.419\n",
            "Loss after mini-batch    21: 0.455\n",
            "Loss after mini-batch    31: 0.462\n",
            "Loss after mini-batch    41: 0.446\n",
            "Loss after mini-batch    51: 0.444\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.032\n",
            "Loss after mini-batch    11: 0.482\n",
            "Loss after mini-batch    21: 0.421\n",
            "Loss after mini-batch    31: 0.434\n",
            "Loss after mini-batch    41: 0.416\n",
            "Loss after mini-batch    51: 0.464\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK: how well does this model do?\n",
        "      # The loss seems to vary from around 0.04 to 0.4, but considering that this is out of 500 values, that seems fairly accurate.\n",
        "# What happens when you change the network architecture? (try different modifications, e.g.: more layers/less layers; wider network; other activation functions). \n",
        "      # I first tried running it a second time, and one of the loss numbers was 0.032.\n",
        "      # I tried changing (13, 64) to (10,64), and it said you cannot multiply the numbers correctly. The same happened after deleting a layer.\n",
        "      # After changing the numbers back, the loss values went up again.\n",
        "      # Changing lr=e-4 to e-3 made the loss values decrease as the epochs increased\n",
        "# What happens when you change the loss function?\n",
        "      # MSELoss gives much larger loss numbers.\n",
        "      # CrossEntropyLoss gives values of 0.\n",
        "# Can you think of ways to visualize your network performance? (plots)\n",
        "      # You could create a plot containing all the values from each epoch as the x values and all the loss values as y coordinates.\n",
        "# Can you split the data into training, validation and test set? With the validation set you can check for overfitting while you are training.\n",
        "# The test set is used for your final model evaluation.\n",
        "      # The training set is the boston data, and the validation set is the california data. I'm not sure about the test value though."
      ],
      "metadata": {
        "id": "JnRFzTa_GSOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}